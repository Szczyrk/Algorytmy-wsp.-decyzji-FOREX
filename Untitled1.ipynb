{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                (None, 8)                 624       \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 8)                 0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 16)                144       \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 16)                0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 3)                 51        \n",
      "=================================================================\n",
      "Total params: 819\n",
      "Trainable params: 819\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Training (1):\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 1.1865\n",
      "Long Trades: 4935 (1390 won), Short Trades: 24340 (6475 won), profit: -677250 (gross gain: 393250, gross loss: -1070500)\n",
      "Evaluating: \n",
      "Long Trades: 0 (0 won), Short Trades: 1 (0 won), profit: -50 (gross gain: 0, gross loss: -50)\n",
      "\n",
      "Training (2):\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.1475\n",
      "Long Trades: 4013 (1146 won), Short Trades: 25732 (6654 won), profit: -707250 (gross gain: 390000, gross loss: -1097250)\n",
      "Evaluating: \n",
      "Long Trades: 0 (0 won), Short Trades: 0 (0 won), profit: 0 (gross gain: 0, gross loss: 0)\n",
      "\n",
      "Training (3):\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.9448\n",
      "Long Trades: 4291 (1227 won), Short Trades: 19204 (5162 won), profit: -535850 (gross gain: 319450, gross loss: -855300)\n",
      "Evaluating: \n",
      "Long Trades: 0 (0 won), Short Trades: 1 (0 won), profit: -50 (gross gain: 0, gross loss: -50)\n",
      "\n",
      "Training (4):\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.9151\n",
      "Long Trades: 4697 (1374 won), Short Trades: 22760 (6049 won), profit: -630550 (gross gain: 371150, gross loss: -1001700)\n",
      "Evaluating: \n",
      "Long Trades: 0 (0 won), Short Trades: 1 (0 won), profit: -50 (gross gain: 0, gross loss: -50)\n",
      "\n",
      "Training (5):\n",
      "1/1 [==============================] - 0s 999us/step - loss: 0.8154\n",
      "Long Trades: 5397 (1555 won), Short Trades: 23562 (6221 won), profit: -670350 (gross gain: 388800, gross loss: -1059150)\n",
      "Evaluating: \n",
      "Long Trades: 0 (0 won), Short Trades: 1 (0 won), profit: -50 (gross gain: 0, gross loss: -50)\n",
      "\n",
      "Training (6):\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.7469\n",
      "Long Trades: 6743 (1914 won), Short Trades: 22232 (5921 won), profit: -665250 (gross gain: 391750, gross loss: -1057000)\n",
      "Evaluating: \n",
      "Long Trades: 0 (0 won), Short Trades: 1 (0 won), profit: -50 (gross gain: 0, gross loss: -50)\n",
      "\n",
      "Training (7):\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.7727\n",
      "Long Trades: 8064 (2291 won), Short Trades: 20753 (5470 won), profit: -664750 (gross gain: 388050, gross loss: -1052800)\n",
      "Evaluating: \n",
      "Long Trades: 0 (0 won), Short Trades: 1 (0 won), profit: -50 (gross gain: 0, gross loss: -50)\n",
      "\n",
      "Training (8):\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.6702\n",
      "Long Trades: 9675 (2802 won), Short Trades: 18993 (5016 won), profit: -651600 (gross gain: 390900, gross loss: -1042500)\n",
      "Evaluating: \n",
      "Long Trades: 0 (0 won), Short Trades: 1 (0 won), profit: -50 (gross gain: 0, gross loss: -50)\n",
      "\n",
      "Training (9):\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.6886\n",
      "Long Trades: 11087 (3073 won), Short Trades: 19991 (5253 won), profit: -721300 (gross gain: 416300, gross loss: -1137600)\n",
      "Evaluating: \n",
      "Long Trades: 0 (0 won), Short Trades: 1 (0 won), profit: -50 (gross gain: 0, gross loss: -50)\n",
      "\n",
      "Training (10):\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.6217\n",
      "Long Trades: 8266 (2390 won), Short Trades: 17866 (4598 won), profit: -607800 (gross gain: 349400, gross loss: -957200)\n",
      "Evaluating: \n",
      "Long Trades: 0 (0 won), Short Trades: 0 (0 won), profit: 0 (gross gain: 0, gross loss: 0)\n",
      "\n",
      "Did not solve after 9 episodes :(\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import math\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from environment import Environment\n",
    "\n",
    "EPOCHS = 10\n",
    "THRESHOLD = 195\n",
    "MONITOR = True\n",
    "\n",
    "class DQN:\n",
    "\tdef __init__(self, env_string, batch_size=128):\n",
    "\t\tself.memory = deque(maxlen=100000)\n",
    "\t\tself.env = Environment('EURUSD_H1.csv')\n",
    "\t\tself.input_size = self.env.input_size\n",
    "\t\tself.action_size = len(self.env.action_space)\n",
    "\t\tself.batch_size = batch_size\n",
    "\t\tself.gamma = 1.0\n",
    "\t\tself.epsilon = 1.0\n",
    "\t\tself.epsilon_min = 0.01\n",
    "\t\tself.epsilon_decay = 0.9998\n",
    "\t\talpha=0.01\n",
    "\t\talpha_decay=0.01\n",
    "\t\tdropout = 0.3\n",
    "\n",
    "\t\t# Init model\n",
    "\t\tself.model = Sequential()\n",
    "\t\tself.model.add(Dense(8, input_dim=self.input_size, activation='tanh', kernel_regularizer=l2(0.01), activity_regularizer=l2(0.01)))\n",
    "\t\tself.model.add(Dropout(dropout))\n",
    "\t\tself.model.add(Dense(16, activation='tanh', kernel_regularizer=l2(0.01), activity_regularizer=l2(0.01)))\n",
    "\t\tself.model.add(Dropout(dropout))\n",
    "\t\tself.model.add(Dense(self.action_size, activation='linear'))\n",
    "\t\tself.model.compile(loss='mse', optimizer=Adam(lr=alpha, decay=alpha_decay))\n",
    "\n",
    "\t\tself.model.summary()\n",
    "\n",
    "\tdef remember(self, state, action, reward, next_state, done):\n",
    "\t\tself.memory.append((state, action, reward, next_state, done))\n",
    "\n",
    "\tdef replay(self, batch_size):\n",
    "\t\tx_batch, y_batch = [], []\n",
    "\t\tminibatch = random.sample(self.memory, min(len(self.memory),batch_size))\n",
    "\n",
    "\t\tfor state, action, reward, next_state, done in minibatch:\n",
    "\t\t\ty_target = self.model.predict(state)\n",
    "\t\t\t# print(\"action: {}, reward: {}\".format(action, reward))\n",
    "\t\t\t#print(y_target[0])\n",
    "\t\t\t# y_target[0][action] = reward if done else reward + self.gamma * np.max(self.model.predict(next_state)[0])\n",
    "\t\t\t#y_target[0] = [0, 0, 0]\n",
    "\t\t\ty_target[0][action] = max(min(reward, 1), 0)\n",
    "\t\t\t#print(y_target[0])\n",
    "\t\t\tx_batch.append(state[0])\n",
    "\t\t\ty_batch.append(y_target[0])\n",
    "\n",
    "\t\tself.model.fit(np.array(x_batch), np.array(y_batch), batch_size=len(x_batch), verbose=1)\n",
    "\n",
    "\tdef train(self):\n",
    "\t\tscores = deque(maxlen=100)\n",
    "\t\tavg_scores = []\n",
    "\t\tfor e in range(EPOCHS):\n",
    "\t\t\tprint(\"Training ({}):\".format(e + 1))\n",
    "\n",
    "\t\t\tstate = self.env.reset()\n",
    "\t\t\tstate = self.preprocess_state(state)\n",
    "\t\t\tdone = False\n",
    "\n",
    "\t\t\twhile not done:\n",
    "\t\t\t\tnext_states = []\n",
    "\t\t\t\taction = self.choose_action(state, self.epsilon)\n",
    "\t\t\t\t# print(action)\n",
    "\t\t\t\tnext_state, rewards, dones = self.env.step(action)\n",
    "\t\t\t\tfor s in next_state:\n",
    "\t\t\t\t\ts = self.preprocess_state(s)\n",
    "\t\t\t\t\tnext_states.append(s)\n",
    "\t\t\t\tself.remember(state, action, rewards[0], next_states[0], dones[0])\n",
    "\t\t\t\tfor i in range(1, len(next_states)):\n",
    "\t\t\t\t\tself.remember(next_states[i-1], action, rewards[i], next_states[i], dones[i])\n",
    "\t\t\t\tstate = next_states[len(next_states) - 1]\n",
    "\t\t\t\tself.epsilon = max(self.epsilon_min, self.epsilon_decay*self.epsilon) # decrease epsilon\n",
    "\t\t\t\tif self.env.next_index >= 70000: break\n",
    "\n",
    "\t\t\tself.replay(self.batch_size)\n",
    "\t\t\tprint(self.env.report())\n",
    "\n",
    "\t\t\t# test on unoptimized data\n",
    "\t\t\tprint(\"Evaluating: \")\n",
    "\n",
    "\t\t\tstate = self.env.reset()\n",
    "\t\t\tself.env.next_index = 70001\n",
    "\t\t\tstate = self.preprocess_state(state)\n",
    "\t\t\tdone = False\n",
    "\n",
    "\t\t\twhile not done:\n",
    "\t\t\t\taction = self.choose_action(state, -1)\n",
    "\t\t\t\tnext_state, reward, done = self.env.step(action)\n",
    "\t\t\t\tstate = self.preprocess_state(next_state[len(next_state)-1])\n",
    "\t\t\t\tif self.env.next_index >= 100000: break\n",
    "\n",
    "\t\t\tprint(self.env.report())\n",
    "\t\t\tprint()\n",
    "\n",
    "\n",
    "\t\tprint('Did not solve after {} episodes :('.format(e))\n",
    "\t\treturn avg_scores\n",
    "\n",
    "\tdef choose_action(self, state, epsilon):\n",
    "\t\tif np.random.random() <= epsilon:\n",
    "\t\t\treturn random.choice(self.env.action_space)\n",
    "\t\telse:\n",
    "\t\t\treturn np.argmax(self.model(state, training=epsilon>0))\n",
    "\n",
    "\n",
    "\tdef preprocess_state(self, state):\n",
    "\t\treturn np.reshape(state, [1, self.input_size])\n",
    "\n",
    "env_string = 'CartPole-v0'\n",
    "agent = DQN(env_string)\n",
    "scores = agent.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Long Trades: 0 (0 won), Short Trades: 0 (0 won), profit: 0 (gross gain: 0, gross loss: 0)\n",
      "pas:  100337\n",
      "short:  0\n",
      "long:  0\n"
     ]
    }
   ],
   "source": [
    "state = agent.env.reset()\n",
    "agent.env.next_index = 0\n",
    "state = agent.preprocess_state(state)\n",
    "done = False\n",
    "long, short, pas = 0, 0 ,0\n",
    "actions = []\n",
    "\n",
    "while not done:\n",
    "\taction = agent.choose_action(state, -1)\n",
    "\tnext_state, reward, done = self.env.step(action)\n",
    "\tif action == 1:\n",
    "\t\tlong += 1\n",
    "\telif action == 0:\n",
    "\t\tpas += 1\n",
    "\telse:\n",
    "\t\tshort += 1   \n",
    "\tactions.append(action)\n",
    "\tstate = next_states[len(next_states) - 1]\n",
    "\tif agent.env.next_index >= len(agent.env.dataset) -1: break\n",
    "\n",
    "print(agent.env.report())\n",
    "print(\"pas: \", pas)\n",
    "print(\"short: \", short)\n",
    "print(\"long: \", long)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
