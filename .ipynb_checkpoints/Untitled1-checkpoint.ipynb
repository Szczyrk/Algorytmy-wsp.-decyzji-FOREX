{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                (None, 8)                 624       \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 8)                 0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 16)                144       \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 16)                0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 3)                 51        \n",
      "=================================================================\n",
      "Total params: 819\n",
      "Trainable params: 819\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Training (1):\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 1.2558\n",
      "Long Trades: 10589 (2985 won), Short Trades: 21252 (5638 won), profit: -729750 (gross gain: 431150, gross loss: -1160900)\n",
      "Evaluating: \n",
      "Long Trades: 0 (0 won), Short Trades: 8509 (3091 won), profit: -116350 (gross gain: 154550, gross loss: -270900)\n",
      "\n",
      "Training (2):\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 1.0623\n",
      "Long Trades: 12318 (3387 won), Short Trades: 20186 (5277 won), profit: -758800 (gross gain: 433200, gross loss: -1192000)\n",
      "Evaluating: \n",
      "Long Trades: 1 (0 won), Short Trades: 8508 (3091 won), profit: -116350 (gross gain: 154550, gross loss: -270900)\n",
      "\n",
      "Training (3):\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.9424\n",
      "Long Trades: 14040 (3935 won), Short Trades: 18274 (4839 won), profit: -738300 (gross gain: 438700, gross loss: -1177000)\n",
      "Evaluating: \n",
      "Long Trades: 8492 (3053 won), Short Trades: 1 (0 won), profit: -119350 (gross gain: 152650, gross loss: -272000)\n",
      "\n",
      "Training (4):\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.7575\n",
      "Long Trades: 21518 (6081 won), Short Trades: 9519 (2577 won), profit: -686050 (gross gain: 432900, gross loss: -1118950)\n",
      "Evaluating: \n",
      "Long Trades: 8492 (3053 won), Short Trades: 1 (0 won), profit: -119350 (gross gain: 152650, gross loss: -272000)\n",
      "\n",
      "Training (5):\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.7041\n",
      "Long Trades: 16849 (4770 won), Short Trades: 14827 (3871 won), profit: -719700 (gross gain: 432050, gross loss: -1151750)\n",
      "Evaluating: \n",
      "Long Trades: 8492 (3053 won), Short Trades: 1 (0 won), profit: -119350 (gross gain: 152650, gross loss: -272000)\n",
      "\n",
      "Training (6):\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.6729\n",
      "Long Trades: 16370 (4683 won), Short Trades: 14823 (3878 won), profit: -703550 (gross gain: 428050, gross loss: -1131600)\n",
      "Evaluating: \n",
      "Long Trades: 8492 (3053 won), Short Trades: 1 (0 won), profit: -119350 (gross gain: 152650, gross loss: -272000)\n",
      "\n",
      "Training (7):\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.6004\n",
      "Long Trades: 15754 (4464 won), Short Trades: 15061 (3973 won), profit: -697050 (gross gain: 421850, gross loss: -1118900)\n",
      "Evaluating: \n",
      "Long Trades: 8493 (3054 won), Short Trades: 0 (0 won), profit: -119250 (gross gain: 152700, gross loss: -271950)\n",
      "\n",
      "Training (8):\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.6021\n",
      "Long Trades: 14488 (4078 won), Short Trades: 15415 (4083 won), profit: -679050 (gross gain: 408050, gross loss: -1087100)\n",
      "Evaluating: \n",
      "Long Trades: 8493 (3054 won), Short Trades: 0 (0 won), profit: -119250 (gross gain: 152700, gross loss: -271950)\n",
      "\n",
      "Training (9):\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.5314\n",
      "Long Trades: 13485 (3789 won), Short Trades: 15578 (4055 won), profit: -668750 (gross gain: 392200, gross loss: -1060950)\n",
      "Evaluating: \n",
      "Long Trades: 0 (0 won), Short Trades: 8508 (3092 won), profit: -116200 (gross gain: 154600, gross loss: -270800)\n",
      "\n",
      "Training (10):\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.5282\n",
      "Long Trades: 12484 (3506 won), Short Trades: 15297 (3958 won), profit: -642650 (gross gain: 373200, gross loss: -1015850)\n",
      "Evaluating: \n",
      "Long Trades: 0 (0 won), Short Trades: 0 (0 won), profit: 0 (gross gain: 0, gross loss: 0)\n",
      "\n",
      "Did not solve after 9 episodes :(\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import math\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from environment import Environment\n",
    "\n",
    "EPOCHS = 10\n",
    "THRESHOLD = 195\n",
    "MONITOR = True\n",
    "\n",
    "class DQN:\n",
    "\tdef __init__(self, env_string, batch_size=128):\n",
    "\t\tself.memory = deque(maxlen=100000)\n",
    "\t\tself.env = Environment('EURUSD_H1.csv')\n",
    "\t\tself.input_size = self.env.input_size\n",
    "\t\tself.action_size = len(self.env.action_space)\n",
    "\t\tself.batch_size = batch_size\n",
    "\t\tself.gamma = 1.0\n",
    "\t\tself.epsilon = 1.0\n",
    "\t\tself.epsilon_min = 0.01\n",
    "\t\tself.epsilon_decay = 0.9998\n",
    "\t\talpha=0.01\n",
    "\t\talpha_decay=0.01\n",
    "\t\tdropout = 0.3\n",
    "\n",
    "\t\t# Init model\n",
    "\t\tself.model = Sequential()\n",
    "\t\tself.model.add(Dense(8, input_dim=self.input_size, activation='tanh', kernel_regularizer=l2(0.01), activity_regularizer=l2(0.01)))\n",
    "\t\tself.model.add(Dropout(dropout))\n",
    "\t\tself.model.add(Dense(16, activation='tanh', kernel_regularizer=l2(0.01), activity_regularizer=l2(0.01)))\n",
    "\t\tself.model.add(Dropout(dropout))\n",
    "\t\tself.model.add(Dense(self.action_size, activation='linear'))\n",
    "\t\tself.model.compile(loss='mse', optimizer=Adam(lr=alpha, decay=alpha_decay))\n",
    "\n",
    "\t\tself.model.summary()\n",
    "\n",
    "\tdef remember(self, state, action, reward, next_state, done):\n",
    "\t\tself.memory.append((state, action, reward, next_state, done))\n",
    "\n",
    "\tdef replay(self, batch_size):\n",
    "\t\tx_batch, y_batch = [], []\n",
    "\t\tminibatch = random.sample(self.memory, min(len(self.memory),batch_size))\n",
    "\n",
    "\t\tfor state, action, reward, next_state, done in minibatch:\n",
    "\t\t\ty_target = self.model.predict(state)\n",
    "\t\t\t# print(\"action: {}, reward: {}\".format(action, reward))\n",
    "\t\t\t#print(y_target[0])\n",
    "\t\t\t# y_target[0][action] = reward if done else reward + self.gamma * np.max(self.model.predict(next_state)[0])\n",
    "\t\t\t#y_target[0] = [0, 0, 0]\n",
    "\t\t\ty_target[0][action] = max(min(reward, 1), 0)\n",
    "\t\t\t#print(y_target[0])\n",
    "\t\t\tx_batch.append(state[0])\n",
    "\t\t\ty_batch.append(y_target[0])\n",
    "\n",
    "\t\tself.model.fit(np.array(x_batch), np.array(y_batch), batch_size=len(x_batch), verbose=1)\n",
    "\n",
    "\tdef train(self):\n",
    "\t\tscores = deque(maxlen=100)\n",
    "\t\tavg_scores = []\n",
    "\t\tfor e in range(EPOCHS):\n",
    "\t\t\tprint(\"Training ({}):\".format(e + 1))\n",
    "\n",
    "\t\t\tstate = self.env.reset()\n",
    "\t\t\tstate = self.preprocess_state(state)\n",
    "\t\t\tdone = False\n",
    "\n",
    "\t\t\twhile not done:\n",
    "\t\t\t\tnext_states = []\n",
    "\t\t\t\taction = self.choose_action(state, self.epsilon)\n",
    "\t\t\t\t# print(action)\n",
    "\t\t\t\tnext_state, rewards, dones = self.env.step(action)\n",
    "\t\t\t\tfor s in next_state:\n",
    "\t\t\t\t\ts = self.preprocess_state(s)\n",
    "\t\t\t\t\tnext_states.append(s)\n",
    "\t\t\t\tself.remember(state, action, rewards[0], next_states[0], dones[0])\n",
    "\t\t\t\tfor i in range(1, len(next_states)):\n",
    "\t\t\t\t\tself.remember(next_states[i-1], action, rewards[i], next_states[i], dones[i])\n",
    "\t\t\t\tstate = next_states[len(next_states) - 1]\n",
    "\t\t\t\tself.epsilon = max(self.epsilon_min, self.epsilon_decay*self.epsilon) # decrease epsilon\n",
    "\t\t\t\tif self.env.next_index >= 70000: break\n",
    "\n",
    "\t\t\tself.replay(self.batch_size)\n",
    "\t\t\tprint(self.env.report())\n",
    "\n",
    "\t\t\t# test on unoptimized data\n",
    "\t\t\tprint(\"Evaluating: \")\n",
    "\n",
    "\t\t\tstate = self.env.reset()\n",
    "\t\t\tself.env.next_index = 70001\n",
    "\t\t\tstate = self.preprocess_state(state)\n",
    "\t\t\tdone = False\n",
    "\n",
    "\t\t\twhile not done:\n",
    "\t\t\t\taction = self.choose_action(state, -1)\n",
    "\t\t\t\tnext_state, reward, done = self.env.step(action)\n",
    "\t\t\t\tstate = self.preprocess_state(next_state[len(next_state)-1])\n",
    "\t\t\t\tif self.env.next_index >= 100000: break\n",
    "\n",
    "\t\t\tprint(self.env.report())\n",
    "\t\t\tprint()\n",
    "\n",
    "\n",
    "\t\tprint('Did not solve after {} episodes :('.format(e))\n",
    "\t\treturn avg_scores\n",
    "\n",
    "\tdef choose_action(self, state, epsilon):\n",
    "\t\tif np.random.random() <= epsilon:\n",
    "\t\t\treturn random.choice(self.env.action_space)\n",
    "\t\telse:\n",
    "\t\t\treturn np.argmax(self.model(state, training=epsilon>0))\n",
    "\n",
    "\n",
    "\tdef preprocess_state(self, state):\n",
    "\t\treturn np.reshape(state, [1, self.input_size])\n",
    "\n",
    "env_string = 'CartPole-v0'\n",
    "agent = DQN(env_string)\n",
    "scores = agent.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Long Trades: 0 (0 won), Short Trades: 0 (0 won), profit: 0 (gross gain: 0, gross loss: 0)\n",
      "pas:  100337\n",
      "short:  0\n",
      "long:  0\n"
     ]
    }
   ],
   "source": [
    "state = agent.env.reset()\n",
    "agent.env.next_index = 0\n",
    "state = agent.preprocess_state(state)\n",
    "done = False\n",
    "long, short, pas = 0, 0 ,0\n",
    "actions = []\n",
    "\n",
    "while not done:\n",
    "\taction = agent.choose_action(state, -1)\n",
    "\tnext_state, reward, done = self.env.step(action)\n",
    "\tif action == 1:\n",
    "\t\tlong += 1\n",
    "\telif action == 0:\n",
    "\t\tpas += 1\n",
    "\telse:\n",
    "\t\tshort += 1   \n",
    "\tactions.append(action)\n",
    "\tstate = next_states[len(next_states) - 1]\n",
    "\tif agent.env.next_index >= len(agent.env.dataset) -1: break\n",
    "\n",
    "print(agent.env.report())\n",
    "print(\"pas: \", pas)\n",
    "print(\"short: \", short)\n",
    "print(\"long: \", long)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
